{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 124481,
     "databundleVersionId": 14703169,
     "sourceType": "competition"
    },
    {
     "sourceId": 13954490,
     "sourceType": "datasetVersion",
     "datasetId": 8894615
    }
   ],
   "dockerImageVersionId": 31192,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Prerequisites",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import tempfile\n",
    "from scipy.ndimage import laplace\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import torchvision.models as models"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:23:44.389974Z",
     "iopub.execute_input": "2025-12-19T18:23:44.390360Z",
     "iopub.status.idle": "2025-12-19T18:23:54.696861Z",
     "shell.execute_reply.started": "2025-12-19T18:23:44.390334Z",
     "shell.execute_reply": "2025-12-19T18:23:54.696295Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Data",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Loading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "PATH = \"/kaggle/input/image-noise/Archive\"\nTRAIN_PATH = os.path.join(PATH, 'train.csv')\nVAL_PATH = os.path.join(PATH, 'validation.csv')\nTEST_PATH = os.path.join(PATH, 'test.csv')\nSAMPLES_PATH = os.path.join(PATH, 'samples')\ndef load_samples():\n    samples = []\n    print(f\"Loading samples and extracting features from: {SAMPLES_PATH}\")\n\n    for filename in tqdm(os.listdir(SAMPLES_PATH)):\n        if filename.endswith('.npy'):\n            image_id = os.path.splitext(filename)[0]\n            image_path_full = os.path.join(SAMPLES_PATH, filename)\n\n            try:\n                image = np.load(image_path_full)\n                image = image.astype(np.float32)\n                samples.append({\"sample\":image, \"id\":image_id})\n            except Exception as e:\n                print(f\"Error processing {filename}: {e}\")\n\n    df_samples = pd.DataFrame(samples)\n    df_samples.set_index('id', inplace=True)\n    return df_samples\n\ndef load():\n    df_train = pd.read_csv(TRAIN_PATH)\n    print(f\"Shape of df_train: {df_train.shape}\")\n    df_val = pd.read_csv(VAL_PATH)\n    print(f\"Shape of df_val: {df_val.shape}\")\n    df_test = pd.read_csv(TEST_PATH)\n    print(f\"Shape of df_test: {df_test.shape}\")\n    df_samples = load_samples()\n    print(f\"\\nShape of df_samples: {df_samples.shape}\")\n    return {\n        \"samples\": df_samples,\n        \"train\": df_train,\n        \"val\": df_val,\n        \"test\": df_test\n    }\n\ndata = load()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:23:54.698451Z",
     "iopub.execute_input": "2025-12-19T18:23:54.698795Z",
     "iopub.status.idle": "2025-12-19T18:25:15.712612Z",
     "shell.execute_reply.started": "2025-12-19T18:23:54.698775Z",
     "shell.execute_reply": "2025-12-19T18:25:15.711877Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## General Dataset Analysis",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Pie Chart of Dataset Distribution",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def display_pie_chart(data, filename):\n    values = [data['train'].shape[0], data['val'].shape[0], data['test'].shape[0]]\n    labels = ['Train', 'Validation', 'Test']\n    \n    def make_autopct(all_values):\n        def my_autopct(pct):\n            total = sum(all_values)\n            val = int(round(pct * total / 100.0))\n            return '{p:.1f}%\\n({v:d})'.format(p=pct, v=val)\n        return my_autopct\n\n    plt.figure(figsize=(8, 6))\n    \n    plt.pie(\n        values, \n        labels=labels, \n        autopct=make_autopct(values),\n    )\n    \n    plt.title(filename)\n    plt.axis('equal') \n\n    plt.savefig(filename)\n    plt.show()\n\ndisplay_pie_chart(data, filename=\"example_distribution.svg\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:25:15.713395Z",
     "iopub.execute_input": "2025-12-19T18:25:15.713653Z",
     "iopub.status.idle": "2025-12-19T18:25:15.902123Z",
     "shell.execute_reply.started": "2025-12-19T18:25:15.713624Z",
     "shell.execute_reply": "2025-12-19T18:25:15.901516Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Label Distribution Pie Charts",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def display_label_distribution(df, filename):\n    label_counts = df['label'].value_counts().sort_index()\n\n    plt.figure(figsize=(4, 4))\n\n    plt.pie(\n        label_counts.values, \n        labels=label_counts.index, \n        autopct='%1.1f%%',  # Show percentages with 1 decimal place\n        startangle=90       # Rotate start to vertical\n    )\n    \n    plt.title(filename)\n    \n    plt.savefig(filename)\n    plt.show()\n\n# Usage\ndisplay_label_distribution(data['train'], filename=\"train_label_dist.svg\")\ndisplay_label_distribution(data['val'], filename=\"val_label_dist.svg\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:25:15.902777Z",
     "iopub.execute_input": "2025-12-19T18:25:15.903091Z",
     "iopub.status.idle": "2025-12-19T18:25:16.052748Z",
     "shell.execute_reply.started": "2025-12-19T18:25:15.903072Z",
     "shell.execute_reply": "2025-12-19T18:25:16.052217Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Global Statistics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def analyze_and_plot_histograms(df,filename):\n    pixel_sum = 0.0\n    pixel_sum_sq = 0.0\n    pixel_count = 0\n    global_min = np.inf\n    global_max = -np.inf\n\n    image_stats = {\n        'means': [],\n        'stds': [],\n        'mins': [],\n        'maxs': []\n    }\n\n    print(f\"Processing {len(df)} images...\")\n\n    for img in tqdm(df['sample']):\n        flat = img.flatten().astype(np.float64)\n        \n        current_min = np.min(flat)\n        current_max = np.max(flat)\n        if current_min < global_min: global_min = current_min\n        if current_max > global_max: global_max = current_max\n        \n        pixel_sum += np.sum(flat)\n        pixel_sum_sq += np.sum(flat ** 2)\n        pixel_count += len(flat)\n\n        image_stats['means'].append(np.mean(flat))\n        image_stats['stds'].append(np.std(flat))\n        image_stats['mins'].append(current_min)\n        image_stats['maxs'].append(current_max)\n\n    total_mean = pixel_sum / pixel_count\n    # Variance = E[x^2] - (E[x])^2\n    total_var = (pixel_sum_sq / pixel_count) - (total_mean ** 2)\n    total_std = np.sqrt(total_var)\n\n    print(f\"Global Stats -> Mean: {total_mean:.2f}, Std: {total_std:.2f}, Min: {global_min}, Max: {global_max}\")\n\n    fig = plt.figure(figsize=(16, 12))\n    \n    ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n    subplots = [\n        (1, 0, 'means', 'Distribution of Image Means'),\n        (1, 1, 'stds',  'Distribution of Image Contrast (Std)'),\n        (0, 0, 'mins',  'Distribution of Image Mins'),\n        (0, 1, 'maxs',  'Distribution of Image Maxs')\n    ]\n    \n    for row, col, key, title in subplots:\n        ax = plt.subplot2grid((2, 2), (row, col))\n        sns.histplot(image_stats[key], kde=True, ax=ax, color='steelblue')\n        ax.set_title(title)\n        ax.set_xlabel(key.capitalize())\n\n    plt.tight_layout()\n    plt.savefig(filename)\n    plt.show()\n\n# Run it\nanalyze_and_plot_histograms(data['samples'], 'global-stats.svg')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T05:54:32.025598Z",
     "iopub.execute_input": "2025-12-19T05:54:32.025868Z",
     "iopub.status.idle": "2025-12-19T05:54:44.125863Z",
     "shell.execute_reply.started": "2025-12-19T05:54:32.025841Z",
     "shell.execute_reply": "2025-12-19T05:54:44.125108Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "def calculate_glcm_correlation_custom_range(image):\n",
    "    min_val, max_val = -59, 56\n",
    "    num_levels = 1120\n",
    "\n",
    "    img_clipped = np.clip(image, min_val, max_val)\n",
    "    img_quantized = ((img_clipped - min_val) / (max_val - min_val) * (num_levels - 1)).astype(np.uint8)\n",
    "\n",
    "    glcm = graycomatrix(\n",
    "        img_quantized, \n",
    "        distances=[1], \n",
    "        angles=[0], \n",
    "        levels=num_levels, \n",
    "        symmetric=True, \n",
    "        normed=True\n",
    "    )\n",
    "\n",
    "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "def get_glcm_contrast(image):\n",
    "    img_norm = (image + 59).astype(np.uint8)\n",
    "    glcm = graycomatrix(img_norm, [1], [0], levels=120, symmetric=True, normed=True)\n",
    "\n",
    "    return graycoprops(glcm, 'contrast')[0, 0]\n",
    "\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "def get_lbp_smoothness_bin(image, radius=1, n_points=8):\n",
    "    img_norm = (image + 59).astype(np.uint8)\n",
    "\n",
    "    lbp = local_binary_pattern(img_norm, n_points, radius, method='default')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(2**n_points + 1), density=True)\n",
    "    return hist[0]\n",
    "\n",
    "def get_lbp_uniform_regularity(image, radius=1, n_points=8):\n",
    "    img_norm = (image + 50).astype(np.uint8)\n",
    "    lbp = local_binary_pattern(img_norm, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(n_points + 3), density=True)\n",
    "    return np.sum(hist[:-1])\n",
    "\n",
    "def get_glcm_homogeneity(image):\n",
    "    img_norm = (image + 59).astype(np.uint8)\n",
    "    \n",
    "    glcm = graycomatrix(img_norm, [1], [0], levels=120, symmetric=True, normed=True)\n",
    "\n",
    "    return graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "\n",
    "image = data['samples'].iloc[3]['sample']\n",
    "print(calculate_glcm_correlation_custom_range(image))\n",
    "print(get_glcm_contrast(image))\n",
    "print(get_lbp_smoothness_bin(image))\n",
    "print(get_lbp_uniform_regularity(image))\n",
    "print(get_glcm_homogeneity(image))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:25:16.053443Z",
     "iopub.execute_input": "2025-12-19T18:25:16.053694Z",
     "iopub.status.idle": "2025-12-19T18:25:16.204110Z",
     "shell.execute_reply.started": "2025-12-19T18:25:16.053677Z",
     "shell.execute_reply": "2025-12-19T18:25:16.203431Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_frequency_data(image):\n",
    "    rows, cols = image.shape\n",
    "    f_transform = np.fft.fft2(image)\n",
    "    f_shift = np.fft.fftshift(f_transform)\n",
    "\n",
    "    magnitude = np.abs(f_shift)\n",
    "    power_spectrum = magnitude**2\n",
    "\n",
    "    center_y, center_x = rows // 2, cols // 2\n",
    "    y, x = np.ogrid[:rows, :cols]\n",
    "    dist_map = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "    \n",
    "    return magnitude, power_spectrum, dist_map\n",
    "\n",
    "MAX_DIST = np.sqrt(128**2 + 128**2)\n",
    "\n",
    "def get_high_frequency_energy(power_spectrum, dist_map, threshold_radius=0.8):\n",
    "    mask = dist_map > (threshold_radius * MAX_DIST)\n",
    "    return np.sum(power_spectrum[mask])\n",
    "\n",
    "def get_spectral_centroid(magnitude, dist_map):\n",
    "    return np.sum(dist_map * magnitude) / (np.sum(magnitude) + 1e-9)\n",
    "\n",
    "def get_radial_profile_slope(magnitude, dist_map):\n",
    "    d_flat = dist_map.ravel().astype(int)\n",
    "    m_flat = magnitude.ravel()\n",
    "\n",
    "    tbin = np.bincount(d_flat, weights=m_flat)\n",
    "    nr = np.bincount(d_flat)\n",
    "    radial_profile = tbin / (nr + 1e-9)\n",
    "\n",
    "    limit = 128 \n",
    "    log_dist = np.log(np.arange(1, limit))\n",
    "    log_power = np.log(radial_profile[1:limit] + 1e-9)\n",
    "    \n",
    "    slope, _ = np.polyfit(log_dist, log_power, 1)\n",
    "    return slope\n",
    "\n",
    "def get_low_frequency_power(power_spectrum, dist_map, threshold_radius=0.1):\n",
    "    mask = dist_map < (threshold_radius * MAX_DIST)\n",
    "    return np.sum(power_spectrum[mask])\n",
    "\n",
    "image = data['samples'].iloc[3]['sample']\n",
    "mag, pwr, dists = prepare_frequency_data(image)\n",
    "\n",
    "features = {\n",
    "    \"hf_energy\": get_high_frequency_energy(pwr, dists),\n",
    "    \"centroid\":  get_spectral_centroid(mag, dists),\n",
    "    \"slope\":     get_radial_profile_slope(mag, dists),\n",
    "    \"lf_power\":  get_low_frequency_power(pwr, dists)\n",
    "}"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:57:57.023206Z",
     "iopub.execute_input": "2025-12-19T18:57:57.023968Z",
     "iopub.status.idle": "2025-12-19T18:57:57.036613Z",
     "shell.execute_reply.started": "2025-12-19T18:57:57.023933Z",
     "shell.execute_reply": "2025-12-19T18:57:57.035834Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def image_stats(img):\n",
    "    hist, _ = np.histogram(img.flatten(), bins=64, density=True)\n",
    "\n",
    "    hist = hist[hist > 0]\n",
    "    ent = -(hist * np.log2(hist)).sum()\n",
    "    gclm_corr = calculate_glcm_correlation_custom_range(img)\n",
    "    glcm_contrast = get_glcm_contrast(img)\n",
    "    mag, pwr, dists = prepare_frequency_data(img)\n",
    "    return {\n",
    "        'mean': np.mean(img),\n",
    "        'std': np.std(img),\n",
    "        'min': np.min(img),\n",
    "        'max': np.max(img),\n",
    "        'kurtosis': kurtosis(img.flatten()),\n",
    "        'skew': skew(image.flatten()),\n",
    "        'entropy': ent,\n",
    "        'gclm_corr': gclm_corr,\n",
    "        'glcm_contrast': glcm_contrast,\n",
    "        'glcm_homo': get_glcm_homogeneity(img),\n",
    "        'lbp_smooth': get_lbp_smoothness_bin(img),\n",
    "        'lbp_uniform': get_lbp_uniform_regularity(img),\n",
    "        'high_freq_enenergy': np.log(get_high_frequency_energy(pwr, dists)),\n",
    "        \"centroid\":  np.log(get_spectral_centroid(mag, dists)),\n",
    "        \"slope\":     get_radial_profile_slope(mag, dists),\n",
    "        \"lf_power\":  np.log(get_low_frequency_power(pwr, dists))\n",
    "    }\n",
    "\n",
    "image = data['samples'].iloc[0]['sample']\n",
    "image_stats(image)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:57:57.504875Z",
     "iopub.execute_input": "2025-12-19T18:57:57.505533Z",
     "iopub.status.idle": "2025-12-19T18:57:57.577057Z",
     "shell.execute_reply.started": "2025-12-19T18:57:57.505513Z",
     "shell.execute_reply": "2025-12-19T18:57:57.576325Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Extraction",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def symlog(x):\n    return np.sign(x) * np.log10(np.abs(x) + 1)\n\ndef preprocess_samples(df):\n    df_copy = df.copy()\n    df_copy['sample'] = df_copy['sample'].apply(symlog)\n    return df_copy\n\ndef display_preprocessed_sample(img, filename):\n    prep_img = symlog(img)\n    print(image_stats(img))\n    print(image_stats(prep_img))\n    \n    # plt.figure(figsize=(4, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(img, cmap='seismic')\n    plt.title('Original Image')\n    plt.axis('off')\n\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(prep_img, cmap='seismic')\n    plt.title('Preprocessed image')\n    plt.axis('off')\n\n    plt.savefig(filename)\n\nimage = data['samples'].iloc[0]['sample']\ndisplay_preprocessed_sample(image, \"preprocessed_image.svg\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:25:16.360298Z",
     "iopub.execute_input": "2025-12-19T18:25:16.360654Z",
     "iopub.status.idle": "2025-12-19T18:25:16.765057Z",
     "shell.execute_reply.started": "2025-12-19T18:25:16.360627Z",
     "shell.execute_reply": "2025-12-19T18:25:16.764368Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### FFT Magnitude",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_spectrum(img, use_log_scale=True):\n",
    "    f_transform = np.fft.fft2(img)\n",
    "    f_shift = np.fft.fftshift(f_transform)\n",
    "    magnitude = np.abs(f_shift)\n",
    "    \n",
    "    if use_log_scale:\n",
    "        magnitude = 20 * np.log(1 + magnitude)\n",
    "\n",
    "    min_val = np.min(magnitude)\n",
    "    max_val = np.max(magnitude)\n",
    "    \n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(magnitude)\n",
    "        \n",
    "    normalized = (magnitude - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def display_spectrum_analysis(img, filename):\n",
    "    spectrum = compute_spectrum(img, use_log_scale=False)\n",
    "    log_spectrum = compute_spectrum(img, use_log_scale=True)\n",
    "\n",
    "    print(f\"Image stats: {image_stats(img)}\")\n",
    "    print(f\"FFT stats: {image_stats(spectrum)}\")\n",
    "    print(f\"Log FFT stats: {image_stats(log_spectrum)}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img, cmap='seismic')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(spectrum, cmap='seismic')\n",
    "    plt.title('Magnitude Spectrum')\n",
    "    plt.axis('off')\n",
    "\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(log_spectrum, cmap='seismic')\n",
    "    plt.title('Log Magnitude Spectrum')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(filename)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "image = data['samples'].iloc[0]['sample']\n",
    "display_spectrum_analysis(image, filename=\"fft_spectrum.svg\")\n",
    "display_spectrum_analysis(symlog(image), filename=\"fft_spectrum_prep.svg\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:25:16.765709Z",
     "iopub.execute_input": "2025-12-19T18:25:16.765906Z",
     "iopub.status.idle": "2025-12-19T18:25:18.613829Z",
     "shell.execute_reply.started": "2025-12-19T18:25:16.765890Z",
     "shell.execute_reply": "2025-12-19T18:25:18.612877Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Radial Profile",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_radial_profile(magnitude):\n    y, x = np.indices(magnitude.shape)\n    center = np.array(magnitude.shape) / 2\n    r = np.sqrt((x - center[1])**2 + (y - center[0])**2)\n    r = r.astype(int)\n\n    tbin = np.bincount(r.ravel(), weights=magnitude.ravel())\n    nr = np.bincount(r.ravel())\n\n    radial_profile = tbin / np.maximum(nr, 1)\n    \n    return radial_profile\n\ndef display_radial_profile(img, filename):\n    magnitude = compute_spectrum(img)\n    profile = get_radial_profile(magnitude)\n    \n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(img, cmap='seismic')\n    plt.title('Original')\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(magnitude, cmap='seismic')\n    plt.title('2D Magnitude Spectrum')\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    plt.plot(profile, color='blue', linewidth=2)\n    plt.title(filename)\n    plt.xlabel('Frequency (Radius)')\n    plt.ylabel('Average Magnitude')\n    plt.grid(True, alpha=0.3)\n\n    plt.suptitle(filename)\n    plt.savefig(filename)\n    plt.tight_layout()\n    plt.show()\n\nimage = data['samples'].iloc[0]['sample']\ndisplay_radial_profile(image, \"radial_profile.svg\")\ndisplay_radial_profile(symlog(image), \"radial_profile_preprocessed.svg\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:25:18.614855Z",
     "iopub.execute_input": "2025-12-19T18:25:18.615480Z",
     "iopub.status.idle": "2025-12-19T18:25:19.821140Z",
     "shell.execute_reply.started": "2025-12-19T18:25:18.615446Z",
     "shell.execute_reply": "2025-12-19T18:25:19.820336Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Pixel Value Histogram",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_pixel_histrogram(img, bins=64):\n    return  np.histogram(img.flatten(), bins=bins, density=True)\n    \ndef display_hist(img, bins, filename):\n    spectrum = compute_spectrum(img)\n\n    pixel_hist, pixel_bin_edges = get_pixel_histrogram(img, bins)\n    fft_hist, fft_bin_edges = get_pixel_histrogram(spectrum, bins)\n\n    plt.figure(figsize=(20, 5))\n\n    plt.subplot(1, 4, 1)\n    plt.imshow(img, cmap='seismic') \n    plt.title(\"Original Image\")\n    plt.axis('off')\n\n    plt.subplot(1, 4, 2)\n    plt.imshow(spectrum, cmap='seismic')\n    plt.title(\"FFT Log Magnitude\")\n    plt.axis('off')\n\n    plt.subplot(1, 4, 3)\n    plt.bar(pixel_bin_edges[:-1], pixel_hist, width=np.diff(pixel_bin_edges), \n            edgecolor='black', align='edge', color='skyblue')\n    plt.title(\"Pixel Intensity Histogram\")\n    plt.xlabel(\"Pixel Value\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis='y', alpha=0.3)\n\n\n    plt.subplot(1, 4, 4)\n    plt.bar(fft_bin_edges[:-1], fft_hist, width=np.diff(fft_bin_edges), \n            edgecolor='black', align='edge', color='orange')\n    plt.title(\"FFT Log-Mag Histogram\")\n    plt.xlabel(\"Log Magnitude\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis='y', alpha=0.3)\n\n    plt.suptitle(filename)\n    plt.tight_layout()\n    plt.savefig(filename)\n    plt.show()\n\nimage = data['samples'].iloc[0]['sample']\ndisplay_hist(image, 256, \"histogram.svg\")\ndisplay_hist(symlog(image), 256, \"histogram_preprocessed.svg\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:25:19.822110Z",
     "iopub.execute_input": "2025-12-19T18:25:19.822396Z",
     "iopub.status.idle": "2025-12-19T18:25:23.386492Z",
     "shell.execute_reply.started": "2025-12-19T18:25:19.822371Z",
     "shell.execute_reply": "2025-12-19T18:25:23.385686Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Full Feature Set",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def display_feature_set(img, filename):\n    print(f\"Image Stats: {image_stats(img)}\")\n    \n    img_sym = symlog(img)\n    \n    spec_orig = compute_spectrum(img)\n    spec_sym = compute_spectrum(img_sym)\n    \n    radial_prof = get_radial_profile(spec_orig)\n    \n    hist, bin_edges = np.histogram(img_sym.flatten(), bins=128, density=True)\n\n    plt.figure(figsize=(28, 5))\n    \n    plt.subplot(1, 6, 1)\n    plt.imshow(img, cmap='seismic')\n    plt.title(\"1. Original Image (Raw)\")\n    plt.axis('off')\n    \n    plt.subplot(1, 6, 2)\n    plt.imshow(img_sym, cmap='seismic')\n    plt.title(\"2. Symlog Transformed Image\")\n    plt.axis('off')\n\n    plt.subplot(1, 6, 3)\n    plt.bar(bin_edges[:-1], hist, width=np.diff(bin_edges), edgecolor='black', align='edge', color='skyblue')\n    plt.title(\"3. Pixel Intensity Histogram\")\n    plt.xlabel(\"Pixel Value\")\n    plt.grid(alpha=0.3)\n\n    plt.subplot(1, 6, 4)\n    plt.imshow(spec_orig, cmap='seismic')\n    plt.title(\"4. Spectrum (Original)\")\n    plt.axis('off')\n\n    plt.subplot(1, 6, 5)\n    plt.imshow(spec_sym, cmap='seismic')\n    plt.title(\"5. Spectrum (Symlog - Cleaner)\")\n    plt.axis('off')\n\n    plt.subplot(1, 6, 6)\n    plt.plot(radial_prof, color='blue', linewidth=2)\n    plt.title(\"6. Radial Profile (of Spectrum)\")\n    plt.xlabel(\"Frequency Radius\")\n    plt.ylabel(\"Avg Magnitude\")\n    plt.grid(alpha=0.3)\n\n    plt.savefig(filename)\n    plt.tight_layout()\n    plt.show()\n\nfor i in range(0, 2):\n    image = data['samples'].iloc[i]['sample']\n    display_feature_set(image, \"full_feature_set.svg\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T19:20:55.564949Z",
     "iopub.execute_input": "2025-12-19T19:20:55.565148Z",
     "iopub.status.idle": "2025-12-19T19:20:58.914495Z",
     "shell.execute_reply.started": "2025-12-19T19:20:55.565131Z",
     "shell.execute_reply": "2025-12-19T19:20:58.913726Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def find_and_display_shared_pairs(df_train, df_samples):\n",
    "    all_ids = pd.concat([df_train['id_noise_1'], df_train['id_noise_2']])\n",
    "    id_counts = all_ids.value_counts()\n",
    "    \n",
    "    reused_ids = id_counts[id_counts >= 2]\n",
    "    \n",
    "    if reused_ids.empty:\n",
    "        print(\"No shared images found (every image appears in only one pair).\")\n",
    "        return\n",
    "\n",
    "    shared_id = reused_ids.index[0]\n",
    "    count = reused_ids.iloc[0]\n",
    "    \n",
    "    mask = (df_train['id_noise_1'] == shared_id) | (df_train['id_noise_2'] == shared_id)\n",
    "    pairs = df_train[mask].head(2)  \n",
    "    print(f\"Found Shared Image ID: {shared_id} (Appears in {count} pairs)\")\n",
    "    \n",
    "    for i, (idx, row) in enumerate(pairs.iterrows()):\n",
    "        id1 = row['id_noise_1']\n",
    "        id2 = row['id_noise_2']\n",
    "        label = row['label']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PAIR {i+1}: Label {label} | IDs: {id1} vs {id2}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        img1 = df_samples.loc[id1]['sample']\n",
    "        img2 = df_samples.loc[id2]['sample']\n",
    "        \n",
    "        print(f\"--- Image 1 (ID: {id1}) {'[SHARED]' if id1 == shared_id else ''} ---\")\n",
    "        display_feature_set(img1, \"img1_set.svg\")\n",
    "        \n",
    "        print(f\"--- Image 2 (ID: {id2}) {'[SHARED]' if id2 == shared_id else ''} ---\")\n",
    "        display_feature_set(img2, \"img2_set.svg\")\n",
    "\n",
    "find_and_display_shared_pairs(data['train'], data['samples'])"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:25:24.962655Z",
     "iopub.execute_input": "2025-12-19T18:25:24.962912Z",
     "iopub.status.idle": "2025-12-19T18:25:31.618555Z",
     "shell.execute_reply.started": "2025-12-19T18:25:24.962893Z",
     "shell.execute_reply": "2025-12-19T18:25:31.617478Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Training - XGBoost",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Create feature vector",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_feature_vector(img):\n    img_sym = symlog(img)\n    spec_orig = compute_spectrum(img)\n    # spec_sym = compute_spectrum(img_sym)\n    \n    stats_orig = np.array(list(image_stats(img).values()))\n    # stats_sym = np.array(list(image_stats(img_sym).values()))\n\n    # hist, _ = np.histogram(img_sym.flatten(), bins=128, density=True)\n    # rad_profile = get_radial_profile(spec_orig)\n    \n    # return np.concatenate([stats_orig, stats_sym, hist, rad_profile])\n    return stats_orig\n\nimage = data['samples'].iloc[0]['sample']\nprint(*get_feature_vector(image))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T18:58:20.169743Z",
     "iopub.execute_input": "2025-12-19T18:58:20.170054Z",
     "iopub.status.idle": "2025-12-19T18:58:20.236035Z",
     "shell.execute_reply.started": "2025-12-19T18:58:20.170030Z",
     "shell.execute_reply": "2025-12-19T18:58:20.235423Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def get_combined_features(f1, f2):\n",
    "    diff = np.abs(f1 - f2)\n",
    "\n",
    "    eps = 1e-8\n",
    "    ratio = np.log((np.abs(f1) + eps) / (np.abs(f2) + eps))\n",
    "\n",
    "    sq_diff = np.square(f1 - f2)\n",
    "\n",
    "    return np.concatenate([f1, f2, diff, ratio, sq_diff])\n",
    "\n",
    "def build_dataset(samples_df, pairs_df):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for _, row in tqdm(pairs_df.iterrows()):\n",
    "        id1, id2 = row['id_noise_1'], row['id_noise_2']\n",
    "        f1 = get_feature_vector(samples_df.loc[id1, 'sample'])\n",
    "        f2 = get_feature_vector(samples_df.loc[id2, 'sample'])\n",
    "\n",
    "        combined = get_combined_features(f1, f2)\n",
    "        \n",
    "        X_list.append(combined)\n",
    "        if 'label' in row:\n",
    "            y_list.append(row['label'])\n",
    "        \n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "\n",
    "X_train, y_train = build_dataset(data['samples'], data['train'])\n",
    "X_val, y_val = build_dataset(data['samples'], data['val'])\n",
    "X_test, _ = build_dataset(data['samples'], data['test'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T19:37:38.725507Z",
     "iopub.execute_input": "2025-12-19T19:37:38.726283Z",
     "iopub.status.idle": "2025-12-19T20:00:24.983568Z",
     "shell.execute_reply.started": "2025-12-19T19:37:38.726257Z",
     "shell.execute_reply": "2025-12-19T20:00:24.982822Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Grid Search",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def xgb_grid_search(param_grid, static_params, history_file):\n",
    "    print(f\"Starting Grid Search with {len(ParameterGrid(param_grid))} combinations...\")\n",
    "\n",
    "    for i, params in enumerate(ParameterGrid(param_grid)):\n",
    "        print(f\"\\n--- Iteration {i+1} ---\")\n",
    "        print(f\"Testing params: {params}\")\n",
    "\n",
    "        current_params = {**static_params, **params}\n",
    "        model = xgb.XGBClassifier(**current_params)\n",
    "        model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val_scaled)\n",
    "        probs = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "        val_acc = accuracy_score(y_val, preds)\n",
    "        val_auc = roc_auc_score(y_val, probs)\n",
    "    \n",
    "        print(f\"Result -> Accuracy: {val_acc:.4f} | AUC: {val_auc:.4f}\")\n",
    "\n",
    "        log_entry = current_params.copy()\n",
    "        log_entry.update({\n",
    "            'val_accuracy': val_acc,\n",
    "            'val_auc': val_auc,\n",
    "            'best_iteration': model.best_iteration,\n",
    "            'best_score': model.best_score,\n",
    "        })\n",
    "    \n",
    "        log_df = pd.DataFrame([log_entry])\n",
    "    \n",
    "        if not os.path.isfile(history_file):\n",
    "            log_df.to_csv(history_file, index=False)\n",
    "        else:\n",
    "            log_df.to_csv(history_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"\\nGrid search complete. Results saved to {history_file}\")\n",
    "\n",
    "    results = pd.read_csv(history_file)\n",
    "    best_run = results.loc[results['val_auc'].idxmax()]\n",
    "    print(\"\\nBest Parameters found by AUC:\")\n",
    "    print(best_run)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'min_child_weight': [1, 5],\n",
    "    'subsample': [0.6, 0.8],\n",
    "    'colsample_bytree': [0.7, 1.0]\n",
    "}\n",
    "\n",
    "static_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'n_jobs': -1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "history_file = 'grid_search_history.csv'\n",
    "xgb_grid_search(param_grid, static_params, history_file)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T19:25:35.073911Z",
     "iopub.execute_input": "2025-12-19T19:25:35.074554Z",
     "iopub.status.idle": "2025-12-19T19:30:08.213137Z",
     "shell.execute_reply.started": "2025-12-19T19:25:35.074529Z",
     "shell.execute_reply": "2025-12-19T19:30:08.212395Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Model Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train(params):\n",
    "    print(\"Training XGBoost...\")\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],\n",
    "        verbose=5\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val_scaled)\n",
    "    probs = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    val_acc = accuracy_score(y_val, preds)\n",
    "    val_auc = roc_auc_score(y_val, probs)\n",
    "    \n",
    "    print(f\"\\nValidation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    history_file = 'experiment_history.csv'\n",
    "    \n",
    "    log_entry = params.copy()\n",
    "    log_entry.update({\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_auc': val_auc,\n",
    "        'best_iteration': model.best_iteration,\n",
    "        'best_score': model.best_score,\n",
    "    })\n",
    "\n",
    "    log_df = pd.DataFrame([log_entry])\n",
    "\n",
    "    if not os.path.isfile(history_file):\n",
    "        log_df.to_csv(history_file, index=False)\n",
    "    else:\n",
    "        log_df.to_csv(history_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"Results saved to {history_file}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 1,\n",
    "    'early_stopping_rounds': 200,\n",
    "    'n_jobs': -1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42 \n",
    "}\n",
    "model = train(params)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T20:01:39.578534Z",
     "iopub.execute_input": "2025-12-19T20:01:39.579231Z",
     "iopub.status.idle": "2025-12-19T20:01:54.011522Z",
     "shell.execute_reply.started": "2025-12-19T20:01:39.579198Z",
     "shell.execute_reply": "2025-12-19T20:01:54.010818Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def train_rf(params):\n",
    "    print(\"Training Random Forest...\")\n",
    "\n",
    "    model = RandomForestClassifier(**params)\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    preds = model.predict(X_val_scaled)\n",
    "    probs = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    val_acc = accuracy_score(y_val, preds)\n",
    "    val_auc = roc_auc_score(y_val, probs)\n",
    "    \n",
    "    print(f\"\\nValidation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    history_file = 'experiment_history_rf.csv'\n",
    "\n",
    "    log_entry = params.copy()\n",
    "    log_entry.update({\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_auc': val_auc,\n",
    "    })\n",
    "    \n",
    "    log_df = pd.DataFrame([log_entry])\n",
    "    \n",
    "    if not os.path.isfile(history_file):\n",
    "        log_df.to_csv(history_file, index=False)\n",
    "    else:\n",
    "        log_df.to_csv(history_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"Results saved to {history_file}\")\n",
    "    return model\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 5,\n",
    "    'max_features': 'sqrt',\n",
    "    'bootstrap': True,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42 \n",
    "}\n",
    "\n",
    "model = train_rf(rf_params)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T20:01:23.301628Z",
     "iopub.execute_input": "2025-12-19T20:01:23.302128Z",
     "iopub.status.idle": "2025-12-19T20:01:24.614025Z",
     "shell.execute_reply.started": "2025-12-19T20:01:23.302103Z",
     "shell.execute_reply": "2025-12-19T20:01:24.613361Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Results Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def validate_extended_metrics(model, X_val, y_val, model_name, filename):\n",
    "    probs = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    mae = mean_absolute_error(y_val, probs)\n",
    "    mse = mean_squared_error(y_val, probs)\n",
    "    \n",
    "    spearman_corr, _ = spearmanr(y_val, probs)\n",
    "    kendall_corr, _ = kendalltau(y_val, probs)\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'Spearman_R': spearman_corr,\n",
    "        'Kendall_Tau': kendall_corr\n",
    "    }\n",
    "    \n",
    "    df_results = pd.DataFrame([results])\n",
    "    \n",
    "    print(\"\\n### Validation Performance Report ###\")\n",
    "\n",
    "    display_df = df_results.style.format({\n",
    "        'MAE': \"{:.4f}\",\n",
    "        'MSE': \"{:.4f}\",\n",
    "        'Spearman_R': \"{:.4f}\",\n",
    "        'Kendall_Tau': \"{:.4f}\"\n",
    "    }).hide(axis='index')\n",
    "    \n",
    "    from IPython.display import display\n",
    "    display(display_df)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=['MAE', 'MSE'], y=[mae, mse], palette=\"Reds\")\n",
    "    plt.title(f\"{model_name}: Error Metrics (Lower is Better)\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, max(mae, mse) * 1.2)\n",
    "    for i, v in enumerate([mae, mse]):\n",
    "        plt.text(i, v, f\"{v:.4f}\", ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x=['Spearman', 'Kendall'], y=[spearman_corr, kendall_corr], palette=\"Greens\")\n",
    "    plt.title(f\"{model_name}: Rank Correlations (Higher is Better)\")\n",
    "    plt.ylabel(\"Correlation Coefficient\")\n",
    "    plt.ylim(0, 1.0) \n",
    "    for i, v in enumerate([spearman_corr, kendall_corr]):\n",
    "        plt.text(i, v, f\"{v:.4f}\", ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    return df_results\n",
    "\n",
    "performance_df = validate_extended_metrics(model, X_val_scaled, y_val, model_name=\"XGB_GridSearch_Best\", filename=\"xgboost-results.svg\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T19:35:08.118636Z",
     "iopub.execute_input": "2025-12-19T19:35:08.119096Z",
     "iopub.status.idle": "2025-12-19T19:35:08.684741Z",
     "shell.execute_reply.started": "2025-12-19T19:35:08.119074Z",
     "shell.execute_reply": "2025-12-19T19:35:08.683894Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_calibration_curve(model, X_val, y_val, n_bins=10):\n",
    "    probs = model.predict_proba(X_val)[:, 1]\n",
    "    prob_true, prob_pred = calibration_curve(y_val, probs, n_bins=n_bins, strategy='uniform')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated', color='black')\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='XGBoost Model', color='tab:blue')\n",
    "    \n",
    "    plt.xlabel(\"Mean Predicted Probability\")\n",
    "    plt.ylabel(\"Fraction of Positives\")\n",
    "    plt.title(\"Calibration Curve (Reliability Diagram)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.histplot(probs, bins=20, kde=True, color='tab:blue', alpha=0.5)\n",
    "    plt.xlabel(\"Predicted Probability\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Predicted Probabilities\")\n",
    "    plt.xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_calibration_curve(model, X_val_scaled, y_val)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T20:01:59.209085Z",
     "iopub.execute_input": "2025-12-19T20:01:59.209355Z",
     "iopub.status.idle": "2025-12-19T20:01:59.679218Z",
     "shell.execute_reply.started": "2025-12-19T20:01:59.209333Z",
     "shell.execute_reply": "2025-12-19T20:01:59.678589Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model.feature_importances_",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T20:05:51.378032Z",
     "iopub.execute_input": "2025-12-19T20:05:51.378774Z",
     "iopub.status.idle": "2025-12-19T20:05:51.387296Z",
     "shell.execute_reply.started": "2025-12-19T20:05:51.378746Z",
     "shell.execute_reply": "2025-12-19T20:05:51.386643Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def run_inference(model, test_df, X, output_file='submission.csv'):\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'id_pair': '(' + test_df['id_noise_1'].astype(str) + ',' + test_df['id_noise_2'].astype(str) + ')',\n",
    "        'label': predictions\n",
    "    })\n",
    "\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Success! Results saved to {output_file}\")\n",
    "    print(\"\\nPreview:\")\n",
    "    print(submission_df.head())\n",
    "    return submission_df\n",
    "\n",
    "run_inference(model, data['test'], X_test_scaled)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-19T19:31:44.108343Z",
     "iopub.execute_input": "2025-12-19T19:31:44.109083Z",
     "iopub.status.idle": "2025-12-19T19:31:44.228187Z",
     "shell.execute_reply.started": "2025-12-19T19:31:44.109057Z",
     "shell.execute_reply": "2025-12-19T19:31:44.227602Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Traininig - Siamese Network",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class NoiseMatchingDataset(Dataset):\n",
    "    def __init__(self, pairs_df, samples_df, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.pairs_df = pairs_df\n",
    "        self.image_lookup = dict(zip(samples_df.index, samples_df['sample']))\n",
    "\n",
    "        # if self.mode == 'train':\n",
    "        #     self.crop = T.RandomCrop(224)\n",
    "        # else:\n",
    "        #     self.crop = T.CenterCrop(224)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs_df.iloc[idx]\n",
    "        \n",
    "        id1 = row['id_noise_1']\n",
    "        id2 = row['id_noise_2']\n",
    "        if 'label' in row:\n",
    "            label = float(row['label']) \n",
    "        else:\n",
    "            label = []\n",
    "        img1_raw = self.image_lookup[id1]\n",
    "        img2_raw = self.image_lookup[id2]\n",
    "\n",
    "        img1 = symlog(img1_raw)\n",
    "        img2 = symlog(img2_raw)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if random.random() > 0.5:\n",
    "                img1 = np.fliplr(img1)\n",
    "            if random.random() > 0.5:\n",
    "                img2 = np.fliplr(img2)\n",
    "\n",
    "            if random.random() > 0.5:\n",
    "                img1 = np.flipud(img1)\n",
    "            if random.random() > 0.5:\n",
    "                img2 = np.flipud(img2)\n",
    "\n",
    "            if random.random() > 0.5:\n",
    "                img1 = np.rot90(img1)\n",
    "            if random.random() > 0.5:\n",
    "                img2 = np.rot90(img2)\n",
    "\n",
    "        fft1 = compute_spectrum(img1.copy())\n",
    "        fft2 = compute_spectrum(img2.copy())\n",
    "\n",
    "        img1_t = torch.tensor(img1.copy(), dtype=torch.float32).unsqueeze(0)\n",
    "        img2_t = torch.tensor(img2.copy(), dtype=torch.float32).unsqueeze(0)\n",
    "        fft1_t = torch.tensor(fft1.copy(), dtype=torch.float32).unsqueeze(0)\n",
    "        fft2_t = torch.tensor(fft2.copy(), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # input1 = torch.cat([img1_t, fft1_t], dim=0)\n",
    "        # input2 = torch.cat([img2_t, fft2_t], dim=0)\n",
    "\n",
    "        return fft1_t, fft2_t, torch.tensor(label, dtype=torch.float32)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-14T11:04:11.367875Z",
     "iopub.execute_input": "2025-12-14T11:04:11.368198Z",
     "iopub.status.idle": "2025-12-14T11:04:11.377816Z",
     "shell.execute_reply.started": "2025-12-14T11:04:11.368153Z",
     "shell.execute_reply": "2025-12-14T11:04:11.377201Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Block 1: 256 -> 128\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Block 2: 128 -> 64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Block 3: 64 -> 32\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Block 4: 32 -> 16\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Block 5: 16 -> 8\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Block 6: 8 -> 4\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # Flattened size: 512 channels * 4 * 4 pixels = 8192\n",
    "        self.flatten_dim = 512 * 4 * 4\n",
    "\n",
    "        # --- Classifier Head ---\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        feat1 = self.forward_one(input1)\n",
    "        feat2 = self.forward_one(input2)\n",
    "        \n",
    "        # L1 Distance\n",
    "        diff = torch.abs(feat1 - feat2)\n",
    "\n",
    "        output = self.fc(diff)\n",
    "        return output"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-14T10:32:43.262783Z",
     "iopub.execute_input": "2025-12-14T10:32:43.263480Z",
     "iopub.status.idle": "2025-12-14T10:32:43.272397Z",
     "shell.execute_reply.started": "2025-12-14T10:32:43.263449Z",
     "shell.execute_reply": "2025-12-14T10:32:43.271665Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def run(model, \n",
    "        data, \n",
    "        epochs=50, \n",
    "        learning_rate=0.001, \n",
    "        base_batch_size=64, \n",
    "        patience=5, \n",
    "        save_path=\"best_noise_model.pth\"):\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        final_batch_size = base_batch_size * torch.cuda.device_count()\n",
    "    else:\n",
    "        print(\"Using single GPU or CPU\")\n",
    "        final_batch_size = base_batch_size\n",
    "        \n",
    "    print(f\"Final Batch Size: {final_batch_size}\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_ds = NoiseMatchingDataset(data['train'], data['samples'], mode='train')\n",
    "    val_ds = NoiseMatchingDataset(data['val'], data['samples'], mode='val')\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=final_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=final_batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "        \n",
    "        for img1, img2, labels in train_loop:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "            \n",
    "        train_acc = 100 * correct / total\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for img1, img2, labels in val_loop:\n",
    "                img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(img1, img2).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                val_loop.set_postfix(val_loss=loss.item())\n",
    "                \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}] Train: {train_acc:.1f}% ({avg_train_loss:.4f}) | \"\n",
    "              f\"Val: {val_acc:.1f}% ({avg_val_loss:.4f}) | LR: {current_lr:.6f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f\"{avg_val_loss}_\"+save_path)\n",
    "            print(f\"  >>> New Best Model Saved to {avg_val_loss}_{save_path} (Loss: {best_val_loss:.4f})\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"  >>> Early Stopping triggered after {patience} epochs without improvement.\")\n",
    "                break\n",
    "    \n",
    "    print(\"Training Complete.\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss_history, label='Training Loss', color='blue')\n",
    "    plt.plot(val_loss_history, label='Validation Loss', color='orange', linestyle='--')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('BCELoss')\n",
    "    plt.title(f'Loss Curve (Best Val: {best_val_loss:.4f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_graph.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "model = SiameseNetwork()\n",
    "run(\n",
    "    model=model,\n",
    "    data=data, \n",
    "    epochs=100,\n",
    "    learning_rate=0.0001,\n",
    "    base_batch_size=64,\n",
    "    patience=10,\n",
    "    save_path=\"my_model_v1.pth\"\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-14T11:19:15.028534Z",
     "iopub.execute_input": "2025-12-14T11:19:15.029336Z",
     "iopub.status.idle": "2025-12-14T11:35:09.473192Z",
     "shell.execute_reply.started": "2025-12-14T11:19:15.029305Z",
     "shell.execute_reply": "2025-12-14T11:35:09.472569Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def load_model(model, path, input_shape=(1, 256, 256), device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    \n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"module.\"):\n",
    "            new_state_dict[k[7:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "            \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    \n",
    "    print(f\"Model loaded successfully from {path}\")\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "loaded_model = load_model(SiameseNetwork(), \"/kaggle/working/0.4609244247277578_my_model_v1.pth\", input_shape=(1, 256, 256), device=device)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-14T08:37:23.887851Z",
     "iopub.execute_input": "2025-12-14T08:37:23.888206Z",
     "iopub.status.idle": "2025-12-14T08:37:24.039539Z",
     "shell.execute_reply.started": "2025-12-14T08:37:23.888145Z",
     "shell.execute_reply": "2025-12-14T08:37:24.038936Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_and_save(model, \n",
    "                     samples_df,\n",
    "                     test_df, \n",
    "                     batch_size=64, \n",
    "                     save_path=\"submission.csv\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Predicting on: {device}\")\n",
    "    \n",
    "    model = load_model(model, \"/kaggle/working/0.47040345668792727_my_model_v1.pth\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_ds = NoiseMatchingDataset(test_df, samples_df, mode='val') \n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    print(f\"Starting prediction on {len(test_ds)} pairs...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img1, img2, _ in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            \n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            preds = (outputs > 0.5).int().cpu().numpy()\n",
    "            \n",
    "            if np.ndim(preds) == 0:\n",
    "                predictions.append(preds.item())\n",
    "            else:\n",
    "                predictions.extend(preds.tolist())\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['id_pair'] = \"(\" + test_df['id_noise_1'] + ',' + test_df['id_noise_2'] + \")\"\n",
    "    df['label'] = predictions\n",
    "    \n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"\\nSuccess! Predictions saved to: {save_path}\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "model = SiameseNetwork()\n",
    "\n",
    "predict_and_save(\n",
    "    model=model,\n",
    "    samples_df = data['samples'],\n",
    "    test_df= data['test'],\n",
    "    save_path=\"submission_v1.csv\"\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-14T11:02:37.950419Z",
     "iopub.execute_input": "2025-12-14T11:02:37.951002Z",
     "iopub.status.idle": "2025-12-14T11:02:47.099525Z",
     "shell.execute_reply.started": "2025-12-14T11:02:37.950970Z",
     "shell.execute_reply": "2025-12-14T11:02:47.098598Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Ensemble",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
